{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Network.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPP2uZF6U4AFv+nS++Qa7kj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Momilijaz96/AlphaFold-V1-PyTorch/blob/main/Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVoxNFJVO1QP"
      },
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras.models import Model\n",
        "# Activation and Regularization\n",
        "from keras.regularizers import l2\n",
        "from keras.activations import softmax\n",
        "from keras import backend as K\n",
        "\n",
        "# Keras layers\n",
        "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
        "from keras.layers import Dense, Dropout, Flatten, Input, BatchNormalization, Activation, Add\n",
        "#Imoprt Tensforflow and json for configuration\n",
        "import json\n",
        "import tensorflow as tf\n",
        "\n",
        "# Avoid python call depth errors\n",
        "import sys\n",
        "sys.setrecursionlimit(5000)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOM6key0YySp"
      },
      "source": [
        "#Function for matching shape of weights\n",
        "def map_weights(keras_weight,alphafold_weight):\n",
        "  try:\n",
        "    assert keras_weight.shape == alphafold_weight.shape\n",
        "    return True\n",
        "  except AssertionError as e:\n",
        "    e.args += (keras_weight.shape, alphafold_weight.shape)\n",
        "    raise\n",
        "    \n",
        "\n",
        "\n",
        "def load_alphafold_ckpt(ckpt_path,model_weights):\n",
        "  \"\"\"\n",
        "  Map alphafold weights to keras model\n",
        "  Arguments:\n",
        "    ckpt_path= tf model ckpt path,ending in .ckpt\n",
        "    model= keras model\n",
        "  Returns:\n",
        "    List of numpy arrays of model weights\n",
        "  \"\"\"\n",
        "\n",
        "  modules=['Deep2D','Deep2DExtra','position_specific_bias']\n",
        "  kM_w={w.name.split(':')[0] : w for w in model_weights} #keras model weights\n",
        "    \n",
        "  for tf_name, tf_shape in tf.train.list_variables(str(ckpt_path)):\n",
        "    tf_var = tf.train.load_variable(str(ckpt_path), tf_name) #Get value of the tf_name var from ckpt file\n",
        "    \n",
        "    main_module,*others=tf_name.split('/') #split name of var on '/' and get remaining address in others pointer\n",
        "    if main_module in modules:  \n",
        "      \n",
        "      if main_module in ['Deep2D','Deep2DExtra']:\n",
        "\n",
        "        if others[0].startswith('conv'):\n",
        "          #Get bias and kernel of conv layer of main_module\n",
        "          if others[1]=='b':\n",
        "            map_name=main_module+'/'+others[0]+'/conv2d/bias'\n",
        "          elif others[1]=='w':\n",
        "            map_name=main_module+'/'+others[0]+'/conv2d/kernel'\n",
        "\n",
        "        elif others[0].startswith('output_reshape'):\n",
        "          if others[1]=='w':\n",
        "            map_name=main_module+'/output_reshape/conv2d/kernel'\n",
        "          elif others[1]==others[0]: #batchnorm\n",
        "            if others[2]=='beta':\n",
        "              map_name=main_module+'/output_reshape/batch_norm/beta'\n",
        "            if others[2]=='moving_mean':\n",
        "              map_name=main_module+'/output_reshape/batch_norm/moving_mean'\n",
        "            if others[2]=='moving_variance':\n",
        "              map_name=main_module+'/output_reshape/batch_norm/moving_variance'\n",
        "              \n",
        "        elif others[0].startswith('res'):\n",
        "          #Get weights of a single residual block setup\n",
        "          res_others=others[0].split('_')\n",
        "          \n",
        "          if len(res_others)==1:\n",
        "            #First Batchnorm layer of residual block\n",
        "            if others[1]=='beta':\n",
        "              map_name=main_module+'/'+others[0]+'/batch_norm/beta'\n",
        "          else:\n",
        "            \n",
        "            if res_others[1]=='1x1': \n",
        "              #set weights for c_up layer of this block\n",
        "              if others[1]=='b': #set bias\n",
        "                map_name=main_module+'/'+res_others[0]+'/c_up/conv2d/bias'\n",
        "              elif others[1]=='w':#set weights\n",
        "                map_name=main_module+'/'+res_others[0]+'/c_up/conv2d/kernel'\n",
        "\n",
        "            elif res_others[1]=='1x1h':\n",
        "              #set weights for c_down layer of this block\n",
        "              if others[1]=='b': #set bias\n",
        "                map_name=main_module+'/'+res_others[0]+'/c_down/conv2d/bias'\n",
        "              elif others[1]=='w':#set weights\n",
        "                map_name=main_module+'/'+res_others[0]+'/c_down/conv2d/kernel'\n",
        "              elif others[1]==others[0]: #batchnorm of c_down\n",
        "                if others[2]=='beta':\n",
        "                  map_name=main_module+'/'+res_others[0]+'/c_down/batch_norm/beta'\n",
        "                if others[2]=='moving_mean':\n",
        "                  map_name=main_module+'/'+res_others[0]+'/c_down/batch_norm/moving_mean'\n",
        "                if others[2]=='moving_variance':\n",
        "                  map_name=main_module+'/'+res_others[0]+'/c_down/batch_norm/moving_variance'\n",
        "\n",
        "            elif res_others[1]=='3x3h':\n",
        "              #set weights for c_dialayer of this block\n",
        "              if others[1]=='w': #set bias\n",
        "                map_name=main_module+'/'+res_others[0]+'/c_dia/conv2d/kernel'\n",
        "              \n",
        "              elif others[1]==others[0]:#batchnorm of 3x3h\n",
        "                if others[2]=='beta':\n",
        "                  map_name=main_module+'/'+res_others[0]+'/c_dia/batch_norm/beta'\n",
        "                if others[2]=='moving_mean':\n",
        "                  map_name=main_module+'/'+res_others[0]+'/c_dia/batch_norm/moving_mean'\n",
        "                if others[2]=='moving_variance':\n",
        "                  map_name=main_module+'/'+res_others[0]+'/c_dia/batch_norm/moving_variance'\n",
        "      \n",
        "      elif main_module=='position_specific_bias':\n",
        "        map_name=main_module+'/b'\n",
        "      \n",
        "      kM_w[map_name]=tf_var if map_weights(kM_w[map_name],tf_var) else None   \n",
        "\n",
        "      \n",
        "  return list(kM_w.values())\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJi-gD2HO8Zk"
      },
      "source": [
        " class AlphaFoldConvLayer(tf.keras.layers.Layer):\n",
        "  \"\"\"Creates a convolution layer followed by a batchnorm\n",
        "  and elu layer, which can be turned off by setting corresponding bool to false.\"\"\"\n",
        "  def __init__(self, num_filters,\n",
        "                    kernel_size,\n",
        "                    non_linearity=True,\n",
        "                    batch_norm=False,\n",
        "                    atrou_rate=1,\n",
        "                    name=None):\n",
        "    super(AlphaFoldConvLayer, self).__init__()\n",
        "    if name is not None:\n",
        "      self._name=name\n",
        "\n",
        "    if batch_norm: #Check BN layer and decide bias addition\n",
        "      use_bias=False\n",
        "    else:\n",
        "      use_bias=True\n",
        "\n",
        "    padding='same'\n",
        "\n",
        "    self.batch_norm = BatchNormalization(scale=False, momentum=0.999, fused=True, name='batch_norm') if batch_norm else None\n",
        "    self.elu = Activation('elu') if non_linearity else None\n",
        "    self.conv = Conv2D(num_filters,kernel_size,strides=1,padding=padding,\n",
        "      data_format='channels_last',kernel_initializer='random_normal',\n",
        "      kernel_regularizer=l2(1e-4),dilation_rate=atrou_rate,use_bias=use_bias,name=\"conv2d\")\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.conv(x)\n",
        "\n",
        "    if self.batch_norm:\n",
        "      x = self.batch_norm(x)\n",
        "\n",
        "    if self.elu:\n",
        "      x = self.elu(x)\n",
        "\n",
        "    return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDxhoLNIPNMu"
      },
      "source": [
        "class AlphaFoldResBlock(tf.keras.layers.Layer):\n",
        "  def __init__(self,\n",
        "              num_filters,\n",
        "              kernel_size,\n",
        "              batch_norm=False,\n",
        "              atrou_rate=1,\n",
        "              dropout_keep_prob=1.0,\n",
        "              name=None):\n",
        "    \"\"\" Make a residual block\n",
        "    Arguments:\n",
        "        num_filters (int): Conv2D number of filters, same as channels of input/output of block\n",
        "        kernel_size (int): Conv2D square kernel dimensions\n",
        "        batch_norm (bool): whether to include batch normalization\n",
        "        atrou_rate (int): dilation rate for the main(3x3 dilated) conv layer of block\n",
        "    Return:\n",
        "        A residual block output tensor\n",
        "    \"\"\"\n",
        "    super(AlphaFoldResBlock, self).__init__()\n",
        "    if name is not None:\n",
        "      self._name = name\n",
        "\n",
        "    self.batch_norm = BatchNormalization(scale=False, momentum=0.999, fused=True, name='batch_norm') if batch_norm else None\n",
        "    self.elu = Activation('elu')\n",
        "\n",
        "    #Downsize to half using a 1x1 conv\n",
        "    self.conv_down = AlphaFoldConvLayer(num_filters//2,1,non_linearity=True,batch_norm=True,name='c_down')\n",
        "\n",
        "    #3x3 dilated convolution layer\n",
        "    self.conv_dilated = AlphaFoldConvLayer(num_filters//2,kernel_size,non_linearity=True,batch_norm=True,name='c_dia')\n",
        "\n",
        "    #Upsize to half using a 1x1 conv\n",
        "    #Note: We use TransposeConv2D for upsampling in Keras\n",
        "    #x=Conv2DTranspose(num_filters,1,padding='same')(x)\n",
        "    self.conv_up = AlphaFoldConvLayer(num_filters, 1, False,name='c_up')\n",
        "\n",
        "    #Dropout\n",
        "    self.dropout = Dropout(1-dropout_keep_prob) if dropout_keep_prob<1.0 else None\n",
        "\n",
        "    #Skip connection\n",
        "    self.skip_connect = Add()\n",
        "\n",
        "  def call(self, input_node):\n",
        "    x = input_node\n",
        "\n",
        "    if self.batch_norm:\n",
        "      self.batch_norm(x)\n",
        "\n",
        "    x = self.elu(x)\n",
        "    x = self.conv_down(x)\n",
        "    x = self.conv_dilated(x)\n",
        "    x = self.conv_up(x)\n",
        "\n",
        "    if self.dropout:\n",
        "      x = self.dropout(x)\n",
        "\n",
        "    x = self.skip_connect([x,input_node])\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DG0mqhr6PQu6"
      },
      "source": [
        "class AlphaFoldResBlockStack(tf.keras.layers.Layer):\n",
        "  def __init__(self,\n",
        "    num_features=40,\n",
        "    num_predictions=1,\n",
        "    num_channels=32,\n",
        "    num_blocks=2,\n",
        "    filter_size=3,\n",
        "    batch_norm=False,\n",
        "    atrou_rates=None,\n",
        "    #channel_multiplier=0,\n",
        "    #divide_channels_by=2,\n",
        "    dropout_keep_prob=1.0,\n",
        "    name=None):\n",
        "    \"\"\"\n",
        "      Make a stack of residual blocks with a conv layer at start and end.\n",
        "    Arguments:\n",
        "      input_node (tensor): from previous layer or input\n",
        "      num_features (int): number of input channels\n",
        "      num_predictions (int):number of channels of final output layer\n",
        "      num_channels (int):Input and output number of channels of \n",
        "                          a single residual block\n",
        "      num_blocks (int):number of residual blocks to stack + 2 conv layers\n",
        "      filter_size (int):size of filter for main conv layer of each residual block\n",
        "      batch_norm (bool): wether to use batch norm in a block or not\n",
        "      atrou_rates (int): dilation rates for each subsequent residual block\n",
        "      dropout_keep_prob (double)= 1 - drop_rate for an optional dropout layer at end of each block\n",
        "      resize_features_with_1x1 (bool): Make start and end conv layer 1x1 or not\n",
        "    Returns:\n",
        "      Output of num_blocks stacked residual blocks\n",
        "    \"\"\"\n",
        "    super(AlphaFoldResBlockStack, self).__init__()\n",
        "\n",
        "    if name is not None:\n",
        "      self._name=name\n",
        "\n",
        "    if atrou_rates is None: atrou_rates = [1]\n",
        "    non_linearity=True\n",
        "    num_filters=num_channels\n",
        "\n",
        "    #Loop over num blocks to stack\n",
        "    self.blocks = []\n",
        "    for i_block in range(0,num_blocks):\n",
        "      #Get the current block's dilation rate\n",
        "      curr_atrou_rate=atrou_rates[i_block % len(atrou_rates)]\n",
        "      \n",
        "      #Add a conv layer for first and last block\n",
        "      is_first_block = (i_block==0)\n",
        "      is_last_block = (i_block==num_blocks-1)\n",
        "      if is_first_block or is_last_block:\n",
        "        #For last block set the output channel size\n",
        "        num_filters=num_predictions if is_last_block else num_channels\n",
        "        self.blocks.append(AlphaFoldConvLayer(num_filters,filter_size,non_linearity=non_linearity,atrou_rate=curr_atrou_rate,name=f'conv{i_block+1}'))\n",
        "      #Add middle residual blocks\n",
        "      else:\n",
        "        self.blocks.append(AlphaFoldResBlock(num_filters,filter_size,batch_norm=batch_norm,\n",
        "                        atrou_rate=curr_atrou_rate,\n",
        "                        dropout_keep_prob=dropout_keep_prob, name=f'res{i_block+1}'))\n",
        "    \n",
        "  def call(self, x):\n",
        "    for block in self.blocks:\n",
        "      x = block(x)\n",
        "\n",
        "    return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OH7tfku-sUp"
      },
      "source": [
        "class PositionBias(tf.keras.layers.Layer):\n",
        "  def __init__(self, bias_size):\n",
        "    super(PositionBias, self).__init__()\n",
        "    self.bias_size = bias_size\n",
        "    self._name='position_specific_bias'\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    main_input_shape = input_shape[0]\n",
        "    self.crop_size_x = main_input_shape[1]\n",
        "    self.crop_size_y = main_input_shape[2]\n",
        "    self.num_bins = main_input_shape[3]\n",
        "\n",
        "    b_init = tf.zeros_initializer()\n",
        "    self.b = tf.Variable(initial_value=b_init(shape=(self.bias_size, self.num_bins), dtype=tf.float32), trainable=True,name='b')\n",
        "\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x, crop_x, crop_y = inputs\n",
        "\n",
        "    # These are required because all inputs are feed in as floats (at least with build())\n",
        "    crop_x = tf.cast(crop_x, tf.int32)\n",
        "    crop_y = tf.cast(crop_x, tf.int32)\n",
        "  \n",
        "    # First pad the biases with a copy of the final value to the maximum length.\n",
        "    max_off_diag = tf.reduce_max(tf.maximum(\n",
        "      tf.abs(crop_x[:, 1] - crop_y[:, 0]), \n",
        "      tf.abs(crop_y[:, 1] - crop_x[:, 0])))\n",
        "    padded_bias_size = tf.maximum(self.bias_size, max_off_diag)\n",
        "    biases = tf.concat([self.b, tf.tile(self.b[-1:, :], [padded_bias_size - self.bias_size, 1])], axis=0)\n",
        "    # Now prepend a mirror image (excluding 0th elt) for below-diagonal.\n",
        "    biases = tf.concat([tf.reverse(biases[1:, :], axis=[0]), biases], axis=0)\n",
        "\n",
        "    # Which diagonal of the full matrix each crop starts on (top left):\n",
        "    start_diag = crop_x[:, 0:1] - crop_y[:, 0:1]  # B x 1\n",
        "\n",
        "    # Relative offset of each row within a crop:\n",
        "    # (off-diagonal decreases as y increases)\n",
        "    increment = tf.expand_dims(-tf.range(0, self.crop_size_y), 0)  # 1 x crop_size_y\n",
        "\n",
        "    # Index of diagonal of first element of each row, flattened.\n",
        "    row_offsets = tf.reshape(start_diag + increment, [-1])  # B*crop_size_y\n",
        "\n",
        "    # Make it relative to the start of the biases array. (0-th diagonal is in\n",
        "    # the middle at position padded_bias_size - 1)\n",
        "    row_offsets += padded_bias_size - 1\n",
        "\n",
        "    # Map_fn to build the individual rows.\n",
        "    # B*cropsizey x cropsizex x num_bins\n",
        "    cropped_biases = tf.map_fn(lambda i: biases[i:i+self.crop_size_x, :], elems=row_offsets, fn_output_signature=tf.float32)\n",
        "    cropped_biases = tf.reshape(cropped_biases, [-1, self.crop_size_y, self.crop_size_x, self.num_bins])\n",
        "\n",
        "    return x + cropped_biases\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhoabOyDPTaW"
      },
      "source": [
        "class AlphaFoldNetwork(tf.keras.Model):\n",
        "  def __init__(self, config):\n",
        "    \"\"\"\n",
        "      Go from input features to the distance predictions.\n",
        "      Arguments:\n",
        "        input_shape(3d input shape tuple): Get input shape to initialize placeholders\n",
        "        config(python nested distionary): Network architecture configuration file\n",
        "      Output:\n",
        "        Model\n",
        "                              \n",
        "    \"\"\"\n",
        "    super(AlphaFoldNetwork, self).__init__()\n",
        "\n",
        "    #Get model's configuration file\n",
        "    network_2d_deep = config['network_2d_deep']\n",
        "    output_dimension = config['num_bins']\n",
        "    num_features = 1878\n",
        "\n",
        "    #Get position specific bias size\n",
        "    self.position_specific_bias_size=config['position_specific_bias_size']\n",
        "\n",
        "    ##### LET'S START ASSEMBLING THE MODEL #####\n",
        "    #220 Residual blocks with dilated convolution, with dilation rates of 4 subsequent \n",
        "    #blocks as [1,2,4,8], we are calling these four stacked blocks as 4-block-group.\n",
        "    #Making 7 4-group-blocks at start of network with channels size of a residual blocks as 256\n",
        "    self.Deep2DExtra = AlphaFoldResBlockStack(\n",
        "                                      num_features=num_features,\n",
        "                                      num_predictions=2*network_2d_deep['num_filters'],\n",
        "                                      num_channels= 2*network_2d_deep['num_filters'],\n",
        "                                      num_blocks=network_2d_deep['extra_blocks'] * network_2d_deep['num_layers_per_block'],\n",
        "                                      filter_size=3,\n",
        "                                      batch_norm=network_2d_deep['use_batch_norm'],\n",
        "                                      atrou_rates = [1,2,4,8],\n",
        "                                      dropout_keep_prob=1.0,\n",
        "                                      name='Deep2DExtra'\n",
        "    ) if network_2d_deep['extra_blocks'] else None\n",
        "        \n",
        "    #doble input feature size for next half of the network\n",
        "    num_features = 2 * network_2d_deep['num_filters']\n",
        "\n",
        "    #Making 48 4-group-blocks at start of network with channels size of a residual blocks as 128\n",
        "    self.Deep2D = AlphaFoldResBlockStack(\n",
        "                                      num_features=num_features,\n",
        "                                      num_predictions=network_2d_deep['num_filters'] if config['reshape_layer'] else output_dimension,\n",
        "                                      num_channels= network_2d_deep['num_filters'],\n",
        "                                      num_blocks=network_2d_deep['num_blocks'] * network_2d_deep['num_layers_per_block'],\n",
        "                                      filter_size=3,\n",
        "                                      batch_norm=network_2d_deep['use_batch_norm'],\n",
        "                                      atrou_rates = [1,2,4,8],\n",
        "                                      dropout_keep_prob=1.0,\n",
        "                                      name='Deep2D'\n",
        "                                      )\n",
        "    #Add a 1x1 conv layer to resize the output contact_pre_logits\n",
        "    #if config.reshape_layer was true then the contact_pre_logits output\n",
        "    #is network_2d_deep.num_filters size sized so change it to num_bins\n",
        "    self.reshape = AlphaFoldConvLayer(\n",
        "          num_filters=output_dimension,\n",
        "          kernel_size=1,\n",
        "          non_linearity=False,\n",
        "          batch_norm=network_2d_deep['use_batch_norm'],\n",
        "          name='Deep2D/output_reshape'\n",
        "    ) if config['reshape_layer'] else None\n",
        "\n",
        "    #Position Specific Biases\n",
        "    if self.position_specific_bias_size:\n",
        "      #self.position_specific_bias=K.variable(np.zeros((config['position_specific_bias_size'],output_dimension),dtype='float32'),name='position_specific_bias')\n",
        "      self.position_specific_bias=PositionBias(config['position_specific_bias_size'])\n",
        "  \n",
        "    \n",
        "  def call(self, input):\n",
        "    x,crop_x,crop_y=input\n",
        "\n",
        "    if self.Deep2DExtra:\n",
        "      x = self.Deep2DExtra(x)\n",
        "\n",
        "    x = self.Deep2D(x)\n",
        "\n",
        "    if self.reshape:\n",
        "      x = self.reshape(x)\n",
        "    \n",
        "    if self.position_specific_bias_size:\n",
        "      biases = self.position_specific_bias([x,crop_x,crop_y])\n",
        "      x += biases # BxDxLxL\n",
        "    return x\n",
        "\n",
        "  def load_weights(self,tf_ckpt):\n",
        "    new_w=load_alphafold_ckpt(tf_ckpt,self.weights)\n",
        "    for i in range(len(new_w)):\n",
        "      if type(new_w[i]).__module__ !='numpy':\n",
        "        new_w[i]=new_w[i].numpy()\n",
        "\n",
        "    self.set_weights(new_w)\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLW_2nebQQTU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}